{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aap2204_hw2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apapaioannou92/BA/blob/HW2/aap2204_hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JCRnqkLnjHE",
        "colab_type": "text"
      },
      "source": [
        "# **IEOR 4650  Business Analytics (Fall 2019)**\n",
        "\n",
        "**Homework 2 [Total points: 60]**\n",
        "\n",
        "** Due: 11:59 PM, October 8**\n",
        "\n",
        "This second homework consists of Python programming practices and the applications to linear regression. \n",
        "\n",
        "The steps to finish in the homework:\n",
        "\n",
        "*   Step 1: Make a copy of the file to your LionDrive.\n",
        "*   Step 2: Work with the copy (an ipynb file).\n",
        "*    <font color='red'>**Step 3: Rename the copy to your_uni_hw2.ipynb (For example: yd4501_hw2.ipynb).**</font>  \n",
        "*   Step 4: Submit the copy on CourseWorks.\n",
        "\n",
        "Before you submit your assignment, make sure to re-run your code from the beginning. (You can do so by first clicking Runtime/Reset All Runtimes and then clicking Runtime/Run all)\n",
        "\n",
        "\n",
        "**All the questions in this homework requires coding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSH_bDnX2ybu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This imports all the packages you need for the homework\n",
        "#Please run this first\n",
        "\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "#import modules\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.formula.api import ols\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MbEozs6mhrd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Setting\n",
        "In this exercise, let's continue working with a dataset which explores the store sales.\n",
        "\n",
        "Before doing the assignment, please study this dataset here https://www.kaggle.com/c/rossmann-store-sales/data\n",
        "\n",
        "\n",
        "For this study, we are interested in finding a good model that gives good prediction performance. \n",
        "\n",
        "* Especially, we are interested in use RMSPE as our accuracy measurement. RMSPE is defined as following:\n",
        "$$RMSPE = \\sqrt{\\frac{\\frac{(y_i-\\widehat{y_i}}{y_i})^2}{N}}$$\n",
        "* In addition, any day and store with 0 actual sales is ignored in scoring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziA1Im85eaIF",
        "colab_type": "code",
        "outputId": "d466040c-1fe0-43c4-9648-923af4ba9934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "#Load the dataset\n",
        "link=\"https://drive.google.com/open?id=1Q6J0q4tlWJ7TajX6hMSvZ8tK3CyDD8QY\"\n",
        "_,id=link.split(\"=\")\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Sales.csv')  \n",
        "SALE = pd.read_csv('Sales.csv')\n",
        "SALE.head(5)\n",
        "\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Customers</th>\n",
              "      <th>Open</th>\n",
              "      <th>Promo</th>\n",
              "      <th>StateHoliday</th>\n",
              "      <th>SchoolHoliday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>5263</td>\n",
              "      <td>555</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>6064</td>\n",
              "      <td>625</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>8314</td>\n",
              "      <td>821</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>13995</td>\n",
              "      <td>1498</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>4822</td>\n",
              "      <td>559</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Store  DayOfWeek        Date  Sales  ...  Open  Promo  StateHoliday SchoolHoliday\n",
              "0      1          5  2015-07-31   5263  ...     1      1             0             1\n",
              "1      2          5  2015-07-31   6064  ...     1      1             0             1\n",
              "2      3          5  2015-07-31   8314  ...     1      1             0             1\n",
              "3      4          5  2015-07-31  13995  ...     1      1             0             1\n",
              "4      5          5  2015-07-31   4822  ...     1      1             0             1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNoNiBH0oH3L",
        "colab_type": "text"
      },
      "source": [
        "*You might see a warning when importing the data. This is because StateHoliday has both values in string and numbers. No worries this for now.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCeAzHTC2czY",
        "colab_type": "code",
        "outputId": "50c93e63-b5a4-4122-ae59-40e42c156ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "link=\"https://drive.google.com/open?id=1g6URzUJnhCLOtg0a9TTHC18KYcqwxvoc\"\n",
        "_,id=link.split(\"=\")\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('myfile_store.csv')  \n",
        "STORE = pd.read_csv('myfile_store.csv')\n",
        "STORE.head(5)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>StoreType</th>\n",
              "      <th>Assortment</th>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <th>CompetitionOpenSinceMonth</th>\n",
              "      <th>CompetitionOpenSinceYear</th>\n",
              "      <th>Promo2</th>\n",
              "      <th>Promo2SinceWeek</th>\n",
              "      <th>Promo2SinceYear</th>\n",
              "      <th>PromoInterval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>1270.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>570.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>1</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>c</td>\n",
              "      <td>c</td>\n",
              "      <td>620.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>29910.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Store StoreType  ... Promo2SinceYear    PromoInterval\n",
              "0      1         c  ...             NaN              NaN\n",
              "1      2         a  ...          2010.0  Jan,Apr,Jul,Oct\n",
              "2      3         a  ...          2011.0  Jan,Apr,Jul,Oct\n",
              "3      4         c  ...             NaN              NaN\n",
              "4      5         a  ...             NaN              NaN\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxg8lfkUe5aw",
        "colab_type": "text"
      },
      "source": [
        "## Q1. [8 points]\n",
        "**[Data Cleaning]**\n",
        "\n",
        "1. Merge two tables based on \"Store\" column\n",
        "\n",
        "2. To save you from the pain of running the models for too long, **keep only records for store 1-100**.\n",
        "\n",
        "3. This study only cares about the accuracy performend on the data with Sales higher than 0.There are pros and cons of keeping 0 Sales records. For this study, let's **delete all the sales records with sales=0**.\n",
        "\n",
        "4. Print out the unique values you have in \"StateHoliday\" column. If you see anything that goes wrong, fix the issue. After that, print out the unique values again in \"StateHoliday\" column. [You might find this [link](https://www.geeksforgeeks.org/python-pandas-series-str-replace-to-replace-text-in-a-series/) useful]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa-KpNvoe5HN",
        "colab_type": "code",
        "outputId": "a2cead1a-a9f4-4dfe-de3a-f3d413a78df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Merging the Sales and Store dfs\n",
        "\n",
        "Sales_Store_merged = SALE.merge(STORE, on = \"Store\", how='left') # since Store is the only 1 common column in both dfs\n",
        "\n",
        "# Store = STORE[:99]\n",
        "Filter_ = (Sales_Store_merged['Store'].isin(range(1,101))) & (Sales_Store_merged['Sales']!=0)\n",
        "# Store.head(5)\n",
        "\n",
        "# Reloading the initial Sales dataset into a df\n",
        "\n",
        "Sales_Store_merged = Sales_Store_merged[Filter_]\n",
        "# Sales.head()\n",
        "\n",
        "# Sales_Store_merged.head()\n",
        "\n",
        "Sales_Store_merged['StateHoliday'].unique()\n",
        "\n",
        "# We observe that 0 appears as a string object but also as an integer object. Sales is a categorical feature, so we will replace the 0 (integer value of 0) with the string '0'\n",
        "\n",
        "# Overwriting column with replaced value of age \n",
        "Sales_Store_merged['StateHoliday'] = Sales_Store_merged['StateHoliday'].replace(0, '0') \n",
        "\n",
        "Sales_Store_merged['StateHoliday'].unique()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0', 'a', 'b', 'c'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma7pJk-bp9kQ",
        "colab_type": "code",
        "outputId": "96f846b8-8f8a-4a07-fd46-701ca5d09c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "Sales_Store_merged.head()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Customers</th>\n",
              "      <th>Open</th>\n",
              "      <th>Promo</th>\n",
              "      <th>StateHoliday</th>\n",
              "      <th>SchoolHoliday</th>\n",
              "      <th>StoreType</th>\n",
              "      <th>Assortment</th>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <th>CompetitionOpenSinceMonth</th>\n",
              "      <th>CompetitionOpenSinceYear</th>\n",
              "      <th>Promo2</th>\n",
              "      <th>Promo2SinceWeek</th>\n",
              "      <th>Promo2SinceYear</th>\n",
              "      <th>PromoInterval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>5263</td>\n",
              "      <td>555</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>1270.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>6064</td>\n",
              "      <td>625</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>570.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>1</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>8314</td>\n",
              "      <td>821</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>13995</td>\n",
              "      <td>1498</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>c</td>\n",
              "      <td>c</td>\n",
              "      <td>620.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>4822</td>\n",
              "      <td>559</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>29910.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Store  DayOfWeek  ... Promo2SinceYear    PromoInterval\n",
              "0      1          5  ...             NaN              NaN\n",
              "1      2          5  ...          2010.0  Jan,Apr,Jul,Oct\n",
              "2      3          5  ...          2011.0  Jan,Apr,Jul,Oct\n",
              "3      4          5  ...             NaN              NaN\n",
              "4      5          5  ...             NaN              NaN\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Aj4jvfd4__",
        "colab_type": "text"
      },
      "source": [
        "## Q2 [4 points]\n",
        "\n",
        "**[Preparation for cross-validation]**\n",
        "\n",
        "Split our data into three segments.Instead of shuffling them first, we will do something different. When working with prediction, we might want to have a model that can give good prediction power for the future sales. Thus, we will split our data in the following way:\n",
        "\n",
        "(1) Training: Time window 2013-01-01 to 2015-03-31  \n",
        "\n",
        "(2) Validation: Time window 2015-04-01 to 2015-05-31 \n",
        "\n",
        "(3) Testing: Time window 2015-06-01 to 2015-07-31\n",
        "\n",
        "Print out how many records you have for each set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwBNzoeSegid",
        "colab_type": "code",
        "outputId": "0b547a0c-280e-4b3a-965b-e9ff9b7fa669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#You might find the following sample code helpful \n",
        "#X=X.astype({\"Date\":\"datetime64\"})\n",
        "#X[\"Date\"]<\"2015-01-02\"\n",
        "\n",
        "# We make sure we tranform the type of the Sales_Store_merged['Date'] column from string to datatime64 object type \n",
        "Sales_Store_merged = Sales_Store_merged.astype({\"Date\":\"datetime64\"})\n",
        "\n",
        "# We create the training set \n",
        "Sales_Store_merged_training = Sales_Store_merged[((Sales_Store_merged['Date'] >= \"2013-01-01\") & (Sales_Store_merged['Date'] <= \"2015-03-31\"))]\n",
        "Sales_Store_merged_training.head()\n",
        "\n",
        "# We create the validation set \n",
        "Sales_Store_merged_validation = Sales_Store_merged[((Sales_Store_merged['Date'] >= \"2015-04-01\") & (Sales_Store_merged['Date'] <= \"2015-05-31\"))]\n",
        "\n",
        "# We create the testing set \n",
        "Sales_Store_merged_testing = Sales_Store_merged[((Sales_Store_merged['Date'] >= \"2015-06-01\") & (Sales_Store_merged['Date'] <= \"2015-07-31\"))]\n",
        "\n",
        "print( len(Sales_Store_merged_training), len(Sales_Store_merged_validation), len(Sales_Store_merged_testing) )"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65631 4715 5245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OIl1Vg2euRn",
        "colab_type": "text"
      },
      "source": [
        "##Q3 [8 points]\n",
        "\n",
        "**[Feature Engineering 1 ]**\n",
        "\n",
        "Usually, people pay great attention to the features that are highly predictive. Making sure that adequate variable transformations are performed on those variables are highly important. \n",
        "\n",
        "* Examine why customer numbers is an important predictor based on the data.\n",
        "\n",
        "* Use 4 histograms show how taking the log-transformation changes the distribution of \"Sales\" and \"Customers\" on the training set. Make layout the 4 histograms following the following format\n",
        "\n",
        "\\begin{bmatrix}\n",
        "    \\text{Sales plot here} &  \\text{log(Sales) plot here} \\\\\n",
        "    \\text{Customer plot here} &  \\text{log(Customer) plot here}\n",
        "\\end{bmatrix}.\n",
        "\n",
        ">You will find this [link](https://matplotlib.org/3.1.0/gallery/subplots_axes_and_figures/subplots_demo.html) useful\n",
        "\n",
        "2. \n",
        "  Based on cross-validation to show which model gives a better performance. \n",
        "  $$log(Sales)=\\beta_0+\\beta_1 log(Customers)+\\epsilon$$ \n",
        "  $$Sales=\\beta_0+\\beta_1 Customers+\\epsilon$$ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLxXPghQFsZV",
        "colab_type": "code",
        "outputId": "4f04ee33-d113-49d8-c4c1-03537ccb23d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "# correlation\n",
        "\n",
        "Sales_Store_merged_ = Sales_Store_merged[['Sales', 'Customers']]\n",
        "\n",
        "correlation_ = Sales_Store_merged_.corr(method = 'pearson')\n",
        "\n",
        "print(correlation_)\n",
        "\n",
        "# Customer numbers is an important predictor based on the data because its correlation with the target variable named Sales is high (83%) \n",
        "\n",
        "fig, axs = plt.subplots(2, 2)\n",
        "axs[0, 0].hist(x=Sales_Store_merged_training['Sales'], bins='auto', color='#0504aa',\n",
        "                            alpha=0.75, rwidth=0.85)\n",
        "axs[0, 0].set_title('Sales histogram plot')\n",
        "axs[0, 1].hist(x=np.log(Sales_Store_merged_training['Sales']), bins='auto', color='#0504aa',\n",
        "                            alpha=0.75, rwidth=0.85)\n",
        "axs[0, 1].set_title('log(Sales) histogram plot')\n",
        "axs[1, 0].hist(x=Sales_Store_merged_training['Customers'], bins='auto', color='#0504aa',\n",
        "                            alpha=0.75, rwidth=0.85)\n",
        "axs[1, 0].set_title('Customer histogram plot')\n",
        "axs[1, 1].hist(x=np.log(Sales_Store_merged_training['Customers']), bins= 'auto', color='#0504aa',\n",
        "                            alpha=0.75, rwidth=0.85)\n",
        "\n",
        "axs[1, 1].set_title('log(Customer) histogram plot')\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.set(xlabel='Sales', ylabel='Frequency')\n",
        "\n",
        "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              Sales  Customers\n",
            "Sales      1.000000   0.832475\n",
            "Customers  0.832475   1.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAEYCAYAAADxmJlCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm4HFWd//H3hx3CksSEQCAxKMii\nKIawzAhjlB3F4PxGhcFhUYgLqIyOgugIIirjuCAojFEZNmVfBAcHAkNgcNgCg+xMwpqQQBIChLAv\n398f5zSpdLr79r23+/b2eT1PP7fqVN2qU9Xn1Lfq1OkqRQRmZmattFKrM2BmZuZgZGZmLedgZGZm\nLedgZGZmLedgZGZmLedgZGZmLedg1GSSHpW061AtU9LOkh5s5Po6jaQJkkLSKq3OSztoZBmUtIek\nyxq0rJC06SCXcaukd9eYfrCkG2tM/5OkgwaTh04n6QxJJ7Q6Hw5GdZC0k6T/kfScpMWS/ixpu1bn\nq5KI+O+I2Lyv+SQdJ+mcochTO/N+6LfvAyeWRiRNkXSnpCWSFkn6L0mbDGF+fgwcP9B/joi9IuLM\nvuZrRODsBs3cDw5GfZC0LvBH4BRgJLAR8F3glVbmq535iqQ75ROw9SLi5jy+KXAW8DVgPWAT4JfA\nG0OYrcuBD0naYAjX2VCuL4mDUd/eBRAR50bEGxHxUkRcHRF3AUh6Zz4bfDqfGf5O0vBKC5K0kqSj\nJT2U579A0sg8bQ1J5+T0ZyXdJmlMjXxtI+mufLV2vqQ18nImS5pbWOdRkp6Q9LykByXtImlP4Bjg\nU5KWSvpLnnespMvz1d9sSYcVlrOmpDMlPSPpfknfKFvPo3lddwEvSFqlsK3PS7pP0scL8x+crzB/\nlrf3YUl/ndPnSFpQq/lE0gxJP8zNNEsk/aG0LyvMW3G7qu2HbiZpdUknSZqXPydJWr0w/RuS5udp\nh5adCe8FXF9Y3DbAIxFxbSTPR8TFEfF4Xtb2km7K3+98Sb+QtFqNfP1Y0uOSnpL0b5LWzNNGSfpj\nXs5iSf8taSWAiHgZuB3Yo4/t/nEuu49I2quQPkPSoXl4U0nX5zq1SNL5Of2GPPtfcjn5VE4/LJen\nxbl8jS0sd/dc356TdGpebmk9xbL/NHCc+jiO5Pr19VznX5D0W0ljlJoZn5d0jaQRVbZ9sqS5ko7J\ny35U0gE19lXF7aq2HxomIvyp8QHWBZ4GziRVxhFl0zcFdgNWB0YDNwAnFaY/Cuyah78C3AxsnOf/\nFXBunvY54ApgLWBlYFtg3Sp5ehS4FRhLulq7H/h8njYZmJuHNwfmAGPz+ATgnXn4OOCcsuXeAJwK\nrEE60CwEPpynnUg6EI3I+b+rtJ5Cnu4ExgFr5rRP5DyuBHwKeAHYME87GHgdOCRv7wnA46Qz69WB\n3YHngbWr7IMZwBPAe4BhwMWl7cnbGcAqdWzXCvuh2z5lZfD4XAbXz+X1f4Dv5Wl7Ak8C787l8Jy8\nHzfN0y8Evl5Y7juAl4GfAR8q/65yGd4RWCV/J/cDRxamF5f9M9JVzkhgHVJd+GGe9kPg34BV82dn\nQIXlnAz8tMq2Hwy8BhyWy9kXgHml/8/l6NA8fC7wrVxe1wB2qpTXPP5hYBEwMZfXU4Ab8rRRwBLg\nb/O2fyXn4dBCnl4HvpSnr0l9x5GbgTGk1pkFwB3A+3Ne/ws4tso+mJzX99O8/A+S6uLmefoZwAl9\nbVel/dDQctrqitIJH2DL/IXNzV/q5cCYKvPuC/xvWSEqHQjuB3YpTNswF9JVgM+QDgzvrSM/jwKf\nLoz/CPi3QsErBaNNc6HdFVi1bBnHUTgIk4LIG8A6hbQfAmfk4YeBPQrTDmXFYPSZPvJ9JzAlDx8M\nzCpM2zoX9DGFtKeBbaosawZwYmF8K+BV0gFnQl7WKnVs13L7oRs/ZWXwIWDvwrQ9gEfz8OnkAFAo\nP8WAMZ180lOYZ0fgAlKAfznXk2onEEcClxbGI69DpIPjOwvT/op01QUpgP6BKgdB0n2s06tMOxiY\nXRhfK693g0I5KgWJs4BpwMYVllMejH4L/KgwvjapLk8ADgRuKkwT6aSwGIwe7+M7q3QcOaAwfjFw\nWmH8S8BlVZY1mXTcGlZIuwD45zx8BsuCUdXtqrQfGvlxM10dIuL+iDg4IjYmnYmPBU4CyJfK5yk1\nhS0hnU2OqrKotwOX5uaGZ0nB6Q3S2c7ZwFXAebmJ5EeSVq2RrScLwy+SCk15vmeTDgDHAQtyPseW\nz5eNBRZHxPOFtMdIZ2Gl6XMK04rDFdMkHah0c7u0ve9h+X3zVGH4pZzn8rQVtqvK+h4jnTWX7/u+\ntqvXjCVtf8ljOa00rdZ3/AzpquUtEXFzRHwyIkaTrlj+hnR1gaR35ea1J3Pd+AGV68ZoUpC4vVBW\n/jOnA/wrMBu4Wqk59+iy/18HeLbGNr9VVyLixTxYqVx9gxQ4bpV0r6TP1FjmcvsxIpaSTp42omw/\nRjqKzy37//K6Us9xpLxu9KeuPBMRLxTGi997vdvVVA5G/RQRD5DOJN6Tk35AOlvYOiLWBT5NKtCV\nzAH2iojhhc8aEfFERLwWEd+NiK2AvwY+SjrDGmx+fx8RO5ECYQD/UppUNus8YKSk4sFmPKkpDGA+\nqXmuZFyl1ZUGJL0d+DVwBPC2iBgO3EP1fTMQxTyMJ53BLSqbp6/t6rXH1s8jlYWS8TkN+v6O7yLf\nQ60kIm4DLmFZ3TgNeADYLNeNY6j8/S8iHUzfXagX60XE2nm5z0fE1yLiHcDHgK9K2qXw/1sCg77f\nFxFPRsRhETGW1Gx+qqr3HFtuP0oaBryNVK6W24+SxPL7FVYsd/05jgzEiJzHkuL3XlRru5rKwagP\nkraQ9DVJG+fxccD+pPZbSGdlS4HnJG0EfL3G4v4N+H4+UCNptKQpefhDkraWtDKpvfk14M1B5n1z\nSR9WukH9MqnCl5b5FDChcCN4DqmZ8IdKnSneC3yWdIYG6bL+m5JG5O08oo/VDyNVroU5L4ew7CDV\nKJ+WtJWktUhNORdFxHI9uerYruX2Qw84F/h2LnujgO+w/Hd8iKQt8z7957L/vZJ0vwF46ycPh0la\nP49vQQoWxbqxBFiap32hUoYi4k3SicvPCsvaSNIeefijSp0LBDxHak14M09bg3RvavqA98iy7flE\nqZ6TrgKD5evLOwqzn0vaV9vk+vUD4JaIeBT4D2BrSfsq9ZQ7HOirt19/jiMD9V1Jq0namXSye2GF\neWptF6y4HxqmVyrgYDwP7ADcIukFUkW7h9SdFVI374mkSvIfpDPDan5Out90taTn87J2yNM2AC4i\nVd77SZ0Fzh5k3lcndTxYRGqqWB/4Zp5WKohPS7ojD+9PavOeB1xKuiF6TZ52PKmp4RHgmpzXqt3b\nI+I+4CfATaQCvDXw50FuT7mzSVepT5Ju4n65yny1tqvSfuhmJwAzSVc5d5Nugp8AEBF/InUGuI7U\nLFYKKq/k6XeQDpalMvssKfjcLWkpqWntUtI9TIB/Av6eVId+DZxfI19HldaZm6muIXXAAdgsjy8l\nladTI+K6PG0fYEZEVDrL76/tSPV8KamefiUiHs7TjgPOzM2In8zl559J927mA+8E9gOIiEWkzjs/\nIjVxbUXa57V+DtKf48hAPEkKsPOA35Hu/T1QPlOt7cqOo7AfGpnBUo8Ss36R9AVgv4j4YJ8zN2f9\nM0gdD37TivX3Aklbkk68Vo+I13Pa7sAXI2LflmYuk3QL8NmIuKfVeakmX3XPJXVAuK6v+Zuw/smk\nulLeVNhWfGVkdZG0oaQPKP1WanPSleGlrc6XNZakjyv95mcE6f7iFaVABBDpN3ZtEYgAImKHdgxE\nSo9NGp6bukr3ym7u4996WtOCkaRxkq5T+rHjvZK+ktNHSpouaVb+OyKnS9LJSj+2ukvSxMKyDsrz\nz1KPP0eqhVYj/S7qedJvGv5A+u2OdZfPkX4O8BDp3kzF+zzWp78i7cNFpKbEfSPipdZmqb01rZlO\n0oakHzjekXsy3U7qO38wqavtiUpdNEdExFGS9ib1ld+bdB/l5xGxg9Kv6mcCk0g3FG8Hto2IZ5qS\ncTMzG3JNuzKKiPn5hif5Nx73k/qqTyE9zYD8t3TJPwU4K5KbgeE5oO0BTI+IxTkATSf9UtzMzLrE\nkDygT9IE0mMrbiH9wn5+nvQk6QefkAJV8Ydgc3NatfTydUwFpgIMGzZs2y222KJxG2A2ALfffvui\n/GPQITFq1KiYMGHCUK3OrKKBlvumByNJa5O6CR4ZEUvSTwWSiAhJDWknjIhppEd5MGnSpJg5c2Yj\nFms2YJIe63uuxpkwYQIu99ZqAy33Te1Np/Q4m4uB30VEqd/8U7n5rXRfaUFOf4Llf/G9cU6rlm5m\nZl2imb3pRHro3v0R8dPCpMuBUo+4g0i9skrpB+ZedTsCz+XmvKuA3fMv/0eQnuZ8VbPybWZmQ6+Z\nzXQfAP6B9OvsO3PaMaQnAlwg6bOkB/KVfsV7Jakn3WzSgz8PAYiIxZK+B9yW5zs+IhY3Md9mZjbE\nmhaMIuJGqj/ob5fyhPxk28OrLOt00uPt28puu85g+jWTW50NM2shHwcaw09gGKTddp3R6iyYWROV\n6rjrenM5GJmZ9ZMDU+M5GA2QC6OZFfmYMDgORmZm1nIORgNQfgbkMyKz7lSrbu+26wzX/QZyMDIz\nK+MTzqHnYNQgLqxm3cV1emg5GJmZDZID1+A5GJmZWcs5GJmZWcs5GJmZWcs5GJl1MElTJc2UNHPh\nwoWtzo7ZgDkY9ZNvVFo7iYhpETEpIiaNHj1kL5U1azgHowZzsDIz679mvlzvdEkLJN1TSDtO0hOS\n7syfvQvTvilptqQHJe1RSN8zp82WdHSz8mtmZq3TzCujM4A9K6T/LCK2yZ8rASRtBewHvDv/z6mS\nVpa0MvBLYC9gK2D/PG/b8xWSmVn9mvlyvRskTahz9inAeRHxCvCIpNnA9nna7Ih4GEDSeXne+xqc\nXTMza6G6rowkbd3AdR4h6a7cjDcip20EzCnMMzenVUtva74qMutMrrutU28z3amSbpX0RUnrDWJ9\npwHvBLYB5gM/GcSyluMurmZmnauuYBQROwMHAOOA2yX9XtJu/V1ZRDwVEW9ExJvAr1nWFPdEXnbJ\nxjmtWnqlZbuLq5m1BV9h9V/dHRgiYhbwbeAo4IPAyZIekPS39S5D0oaF0Y8DpZ52lwP7SVpd0ibA\nZsCtwG3AZpI2kbQaqZPD5fWuz8zMOkNdHRgkvRc4BPgIMB3YJyLukDQWuAm4pML/nAtMBkZJmgsc\nC0yWtA0QwKPA5wAi4l5JF5A6JrwOHB4Rb+TlHAFcBawMnB4R9w54a83MrC3V25vuFOA3wDER8VIp\nMSLmSfp2pX+IiP0rJP+22goi4vvA9yukXwlcWWc+28puu85g+jWTW50NM6uDm9Zaq95g9BHgpcLV\nykrAGhHxYkSc3bTcmZlZT6j3ntE1wJqF8bVympmZ2aDVG4zWiIilpZE8vFZzstSefAlv1r2aUb99\nzOifeoPRC5ImlkYkbQu8VGN+MzOzutV7z+hI4EJJ8wABGwCfalquzMysp9QVjCLiNklbAJvnpAcj\n4rXmZat7uEedmVnf+vPU7u2A9wITSU/PPrA5WTIz6w6+b1S/en/0ejbpmXJ3Am/k5ADOalK+zMys\nh9R7z2gSsFVERDMzY2ZmvaneZrp7SJ0WetJgL7V9qW7Wvlw/20O9wWgUcJ+kqyRdXvo0M2NmZs02\nFIHIwa4+9TbTHdfMTJiZWW+r931G15Oesr1qHr4NuKOJ+TIz6xq+Oupbva8dPwy4CPhVTtoIuKxZ\nmTIzazYHiPZS7z2jw4EPAEvgrRftrd+sTJlZfSRNlTRT0syFCxe2OjtmA1ZvMHolIl4tjUhahfQ7\no6oknS5pgaR7CmkjJU2XNCv/HZHTJelkSbMl3VX2HLyD8vyzJB3Uv80bPJ89WTuLiGkRMSkiJo0e\nPbrV2TEbsHqD0fWSjgHWlLQbcCFwRR//cwawZ1na0cC1EbEZcG0eB9iL9KrxzYCpwGmQghfpDbE7\nANsDx5YCmJlZJ/GJbW31BqOjgYXA3aRXhV8JVHzDa0lE3AAsLkueApyZh88E9i2knxXJzcBwSRsC\newDTI2JxRDxDeuV5eYDrCC6IZu3D9bH91Pug1DeBX+fPYIyJiPl5+ElgTB7eCJhTmG9uTquWvgJJ\nU0lXVYwfP36Q2TQzs6FU77PpHqHCPaKIeMdAVxwRIalhjxeKiGnANIBJkyb5sUVmZh2kP8+mK1kD\n+AQwcgDre0rShhExPzfDLcjpTwDjCvNtnNOeACaXpc8YwHrNzKyN1fuj16cLnyci4iTgIwNY3+VA\nqUfcQcAfCukH5l51OwLP5ea8q4DdJY3IHRd2z2lmZtZF6m2mm1gYXYl0pVTzfyWdS7qqGSVpLqlX\n3InABZI+CzwGfDLPfiWwNzAbeBE4BCAiFkv6HumJDwDHR0R5pwgzM+tw9TbT/aQw/Drp0UCfrDxr\nEhH7V5m0S4V5g/TD2krLOR04va5cmpnV4Dcvt696e9N9qNkZMTOz3lVvM91Xa02PiJ82Jjvdz2dm\nZmYr6k9vuu1IHQ0A9gFuBWY1I1NmZtZb6g1GGwMTI+J5AEnHAf8REZ9uVsbMzKx31Ps4oDHAq4Xx\nV1n29ISu5UeGmHUP1+f2Vm8wOgu4VdJx+aroFpY9Y87MzPrBgXFF9f7o9fuk3/48kz+HRMQPmpmx\nbuaCaGY+Diyv3isjgLWAJRHxc2CupE2alCczs4bygb/91fva8WOBo4Bv5qRVgXOalSkzM+st9V4Z\nfRz4GPACQETMA9ZpVqbMzKy31BuMXs2P7AkAScOalyUzs8ZxE11nqDcYXSDpV6Q3sB4GXMPgX7Rn\nZtZU7R6I2j1/Q6ne3nQ/Bi4CLgY2B74TEac0M2O9wAXRzCzp8wkMklYGrskPS53e/CyZmfUOP68y\n6fPKKCLeAN6UtF6jVirpUUl3S7pT0sycNlLSdEmz8t8ROV2STpY0W9JdZe9WahpftZiZDZ16n023\nFLhb0nRyjzqAiPjyINb9oYhYVBg/Grg2Ik6UdHQePwrYC9gsf3YATst/zXqepKnAVIDx48e3ODdm\nA1dvMLokf5ppCunNsJAeNTSDFIymAGfl3nw3SxouacP8WnKznhYR04BpAJMmTYoWZ6etuHWjs/T1\n6vDxEfF4RDT6OXQBXC0pgF/lCjWmEGCeZNmDWDcC5hT+d25OWy4YNfIMcSgLcWldbjM2612+b9T3\nPaPLSgOSLm7geneKiImkJrjDJf1NcWLxN031iohpETEpIiaNHj26gVk1M7Nm6ysYqTD8jkatNCKe\nyH8XAJcC2wNPSdoQIP9dkGd/AhhX+PeNc5qZWUVuous8fQWjqDI8YJKGSVqnNAzsDtxDeovsQXm2\ng4A/5OHLgQNzr7odgee68X6RK49Zb+v1Y0BfHRjeJ2kJ6QppzTxMHo+IWHcA6xwDXCqptP7fR8R/\nSrqN9KSHzwKPAZ/M818J7A3MBl4kvcrCzMy6SM1gFBErN3qFEfEw8L4K6U8Du1RID+DwRufDzMza\nR3/eZ2RDoNcv1c2sNzkYmZlZyzkYmVlX2G3XGW5Z6GAORmVcmM2slXr1GORg1IZ6tTCaWe9yMGpj\nDkpm9XFd6XwORmbWsbr1PlE3blNfHIzMzNpQtwbaahyMOkAvFUgz6031vs/IWsSByGxFrhfdx1dG\nWbsX7nbPn5k1R6801zkYdZBeKZRm1fRy+e/2bXcwMrOO0O0H417nYNSBXCmtV5TKust893MwonML\neqnZrlPzb1aLA1Ft3bZfOiYYSdpT0oOSZks6ulHL7ZYv1IHJuoEDUN+K9byb9lNHBCNJKwO/BPYC\ntgL2l7RVa3PVvqoV1G4quNb5iuXUJ1ID1y37r1N+Z7Q9MDu/JRZJ5wFTgPsGs9BO//LqUS0gTb9m\ncsXhkvLpZv1Rq24Vy1Yv1MGhVtynpX1d6W+76ZRgtBEwpzA+F9ihOIOkqcDUPLpU0oNlyxgFLKq1\nEql2Jho5vcK8o6Rl+WtxXsqn97nvWqid8/Z2SVMjYlqzVlBHuW+mAe37JpXtt/LSzLrTj/+tuG+G\nsl4Xx0t1uDDeTG8fyD91SjDqU67wVSu9pJkRMWkIs9Qv7Zw/523gJM2kRrkcrL7KfTO1075vp7yA\n8zMQHXHPCHgCGFcY3zinmZlZF+iUYHQbsJmkTSStBuwHXN7iPJmZWYN0RDNdRLwu6QjgKmBl4PSI\nuLefi2lJU0Y/tHP+nLeBa/f8DUY7bVs75QWcn35TRLQ6D2Zm1uM6pZnOzMy6mIORmZm1nIORmZm1\nnIORmZm1nIORmZm1nIORmZm1nIORmZm1nIORmZm1nIORmZm1nIORmZm1nIORmZm1nINRj5B0nKRz\naky/V9LkIcxS25E0Q9Khrc5HLZIelbRrg5a1h6TLGrGsTiLpS5L+pY95QtKmVaYdIOnq5uSuM0ia\nLGluI5fZk8FI0t9LmilpqaT5kv4kaadBLrPmwb7dRcS7I2JGrXkkTciVtCOe9t4sXbQfvg+cWBpR\n8mVJ90h6QdJcSRdK2nowK5F0hqQTBp3bxvk1cICk9QfyzxHxu4jYva/52nC7W6Le/dBzwUjSV4GT\ngB8AY4DxwKnAlFbmq5E69SDZqfnuRJK2A9aLiJsLyT8HvgJ8GRgJvAu4DPjI0OewOSStEhEvA38C\nDmx1fgaqK+tKRPTMB1gPWAp8osY8ZwAnFMYnA3ML40eR3jL7PPAgsAuwJ/Aq8Fpe/l/yvGNJLwFc\nDMwGDiss5zjgQuCcvKy7SZX/m8ACYA6we1nefwvMz+s/AVg5TzsY+DPwM+DpYv7L1ncBcFZe373A\npML0R4Fd8/D2wExgCfAU8NOc/jgQeRuXAn9FOqH5NvBYzvdZpINcabkH5mlPA/9ctp7jgIvyPlgC\nHJrXfRPwbN7WXwCrFZYXwBeBWXk7vge8E/ifvIwLivOX7YPSfvoF8BzwALBLYfoM4NA8XHW7Ku2H\nISzDxf23OunEal7+nASsXpj3G3kfzsv7NoBN87TvAL8pzLsZ8AawfY11v7V/CvvzxjysXP4W5O/h\nbuA9wFRSvXg176sr8vxb5uU9SyqLHyurg6eSAsbS/J1tkLfvmfy9vb8w/1jgYmAh8Ajw5bJyv1wZ\ny+kHANfV2NYAPp/L2bPAL1n2yp1mbffbgCvycm4j1fEby/J0eM7TIznt56RjxRLgdmDngR5jqpS1\nbwL35f3+78AaVY6LFber2n6ouL6hqkTt8CEFjdeBVWrMcwZVghGwef4Cx+bxCcA7C1/8OWXLuoFU\nqdYAtsmV5cOF+V8G9iC95PAsUkX6FrAqcFipwOX5LwV+BQwD1gduBT5XqByvA1/Ky1qzwnaV1rc3\n6QWFPwRuLit4pYPcTcA/5OG1gR0L2xvF/Qd8hhRo35HnvQQ4O0/bKhfAnYDVgB/nglkMRq8B+5IO\n/msC2wI75u2YANwPHFlWIf8ArAu8G3gFuDavfz1SxTmoyndb2k//mPfxp0hBaWSePoNlB6ta27XC\nfhjCMlz8no4Hbs7lYTQpIH+vUNafzPtoLdIBqRiMLgS+Xlju54HH+lj3W/unsD9LB+U9SAfD4aQD\n9JbAhlXq1Kp53x6Ty8WHSQfLzQvzL8plYQ3gv0h140BS2T2BHEhyubmdFFxXy9/Xw8Ae1cpYTp8I\nLK6xrQH8MW/PeFLd3bPJ231e/qxFqjtzWDEYTSddtZa249OkILYK8LX8na9R2Pa6jzFVyto9wLi8\nzj+Xtoflj4v1fJ8rnCCXf3qtme5twKKIeH2A//8G6Wx0K0mrRsSjEfFQpRkljQM+ABwVES9HxJ3A\nb1i+aeC/I+KqnJ8LSQeUEyPiNVKhnCBpuKQxpCByZES8EBELSGdj+xWWNS8iTomI1yPipSr5vzEi\nroyIN4CzgfdVme81YFNJoyJiaSzflFPuANKV08MRsZR0JrVfbkb4O9KZ0I0R8SrpgFH+NsebIuKy\niHgzIl6KiNsj4ua8HY+SAvAHy/7nRxGxJNLbfu8Brs7rf450Nv3+GvldAJwUEa9FxPmkq9tKzVC1\ntqtdHAAcHxELImIh8F3gH/K0TwL/HhH3RsSLpANT0XDSAaPkbaSrqIF6DVgH2IJ0BXF/RFRb3o6k\nAH9iRLwaEf9FOvDvX5jn0lwWXiadiL0cEWflsns+y77j7YDREXF8XtbDpHtCxbqxXBnLac+TTl5q\nOTEino2Ix4HrSCeUTdluSSsD/w84NiJejIj7gDMrLOOHEbG4tB0RcU5EPJ3ry09Ix6fNC/PXdYyp\nsQ9+ERFzImIx6R7j/hXmqef77FOvBaOngVEDPaBExGzgSFLFXiDpPEljq8w+lnTmVazwjwEbFcaf\nKgy/RAqUbxTGIX3JbyedfcyX9KykZ0kH6eIN2Dl1bMKTheEXgTWq7IvPki7nH5B0m6SP1ljmWNJ2\nlTxGOgsbk6e9la98UHy67P+Xy7ekd0n6o6QnJS0h3dsbVfY/5futfHztGvl9IvLpWiG/lb7DWtvV\nLirlcWxhWnHflpePZ0gH0ZKngQ0HmpF8APoFqTlrgaRpktatke85EfFmWd5r1Y1q3/HbgbGlepHr\nxjEs/z1VqhvrkK6KaymvLyuUqwZu92hS+ar1na2QJumfJN0v6bm87euxfH2p9xhTTXF9tepKX99n\nn3otGN1EatbZt8Y8L5Auk0s2KE6MiN9HxE6kShBAqYto+Rn/PGCkpGKFH0+639Nfc3K+R0XE8PxZ\nNyLeXczaAJZbUUTMioj9ScHuX4CLJA2rso55pH1RMp7UFPYU6Ux749IESWuSzsCXW13Z+GmkewKb\nRcS6pAOLBr41K9hIUnF540nbUK7WdjVsXw9SpTyWtmW5fU9qaim6i3TCUXItsLGkSTXW11fdODki\ntiU1Mb0L+HppUoV8j5NUPP4Mpm48UqgXwyNinYjYu5i1Cv+3JfCXAaxvBQ3a7oWk8lXrO1tumZJ2\nJt0X/CQwIiKGkwJsI+tLMQ+16kqt77Ou+tJTwSg343wH+KWkfSWtJWlVSXtJ+lGe7U5gb0kjJW1A\nuhICQNLmkj4saXVSW+xLQOk5VAQpAAATK0lEQVRs4CnSJe9KeV1zSG34P5S0hqT3kq44+t39O1/2\nXw38RNK6klaS9E5J5c1XDSHp05JG5zOdZ3Pym6QK8yapXb7kXOAfJW0iaW3Slcz5uVngImAfSX8t\naTXSFWVfFWUd0s3YpZK2AL7QqO3K1ge+nL/3T5AOSldWmK/WdlXaD61wLvBtSaMljSKV7VL5ugA4\nRNKWktYidR4pupJC82dEzCLd3zw3/4ZktVxu95N0dJ7tTuBvc73ZlFSegdQ7T9IOklYlBa2XWb5u\nFPfVLaQrjW/k72EysA+p2ai/bgWel3SUpDUlrSzpPbm3YC0fJDXpDkqjtjtfrVwCHJf37xb03dtv\nHVIAWwisIuk7pHupjXS4pI0ljSTdazq/wjx9fZ/l+6GingpGALld9auknlILSWdWR5C6sEK6l/IX\n0s27q1l+569O+l3GItIl/PqkewmQ2mMBnpZ0Rx7en3Szex6p3fvYiLhmgFk/kHRzsNSz5SIG0azS\nhz2BeyUtJfXW2S/fz3mR1G7859wksiNwOmmf3UC6OfoyqSMF+Z7Ol0iFcj6pM8MC0lVeNf8E/D2p\nTf/XVC78g3ELqefYorwtfxcR5U2HUHu7Ku2HVjiB1OvxLlJPqTtyGhHxJ+Bk0r2O2aSODpD3fUTc\nATwnaYfC8r7MsianZ4GHgI+TenhBuk/5Kungcibwu8L/rkv6vp5hWe/Jf83Tfku6z/qspMvy/cN9\ngL1I38OpwIER8UB/d0A+iH+UdD/nkby831DjfpCkNUj3YCvdk+mvRm73ETnfT5LK3rnUritXAf8J\n/F9e98vU11zfH78nHQcfJpWHFX4vVMd2Lbcfqq2o1FXRrOnyFcazpCa4R1qw/oNJvcEG9QPnTiRp\nS1Jnj9Xz1R2Sdge+GBG1mq27jqQvAeMi4hutzkstSk+J2CAiDmrR+h8l1ZeBnkD3Szv1DLIuJGkf\n0v0Ikbp230266rQmk/RxUnPcWqR7f1dEoSdpRFxNOuvtKRFxSqvzUElumluNVEe2IzWDtvXjqRqp\n55rpbMhNYdmPMjcjNfn5cnxofI7ULPoQ6WcJjb7/Zo21Dum+0Quk5umfkH5T1xPcTGdmZi3nKyMz\nM2s5ByMzM2u5ruzAMGrUqJgwYUKrs2E97vbbb18UEaObuQ5JU0kPo2TYsGHbbrHFFs1cnVmfBlru\nuzIYTZgwgZkzZ7Y6G9bjJD3W91yDExHTgGkAkyZNCpd7a7WBlns305mZWcs5GJmZWcs5GDXBbrvO\naHUWzGwQXIeHnoORmZm1nINRg/mMysys/5oWjCSNk3SdpPsk3SvpKzl9pKTpkmblvyNyuiSdLGm2\npLskTSws66A8/yxJLXlooJn1Hp9cDp1mXhm9DnwtIrYivZb2cElbAUcD10bEZqQHaJbelbIX6dll\nm5F+N3EapOAFHAvsAGwPHFsKYGZm1h2aFowiYn5+Zwr51dv3k15DO4Vl7xE5k2VvXZ0CnBXJzcBw\nSRsCewDT83vfnwGmk963Y2Y25Hy11BxDcs9I0gTg/aQXm43Jby6F9BKp0rvqN2L5F0PNzWnV0ttO\neSF1oTUzq0/Tg1F+odrFwJERsaQ4Lb9KoCGPDZc0VdJMSTMXLlzYiEWamdkQaWowyu+Fvxj4XURc\nkpOfys1v5L8LcvoTwLjCv2+c06qlLycipkXEpIiYNHp0Ux8HZmY9wC0bQ6uZvelEevf5/RHx08Kk\ny4FSj7iDWPbyqMuBA3Ovuh2B53Jz3lXA7pJG5I4Lu+c0M7OWcKBqvGZeGX0A+Afgw5LuzJ+9gROB\n3STNAnbN45Bej/wwMBv4NfBFgIhYDHwPuC1/js9pHcGF1qyz1Ftnd9t1hut3AzXtqd0RcSOgKpN3\nqTB/AIdXWdbpwOmNy13juVCamQ2cn8BgZpb5pLJ1HIzMzKzlHIzMzKzlHIyGgC/9zcxqczAyM8Mn\nja3mYDREXNDNulOpbruOD05dwUjS1s3OSCfrz+8SzMxsRfVeGZ0q6VZJX5S0XlNzZGZmPaeuYBQR\nOwMHkJ4Rd7uk30varak5MzPrEG71GLy67xlFxCzg28BRwAeBkyU9IOlvm5U5M7NmcyBpD/XeM3qv\npJ+RXpD3YWCfiNgyD/+sifkzM+sYDmwDV++V0SnAHcD7IuLwwhtc55GulszMDAekgao3GH0E+H1E\nvAQgaSVJawFExNnNypyZWadyUOqfeoPRNcCahfG1cpqZWcdz4Gi9eoPRGhGxtDSSh9dqTpbMzKzX\n1BuMXpA0sTQiaVvgpeZkyczMek29wehI4EJJ/y3pRuB84IjmZau7uUnAzGx5db3pNSJuk7QFsHlO\nejAiXmtetszMOlfxeXXTr5nc0rx0iv68dnw7YEL+n4mSiIizmpIrMzPrKfX+6PVs4MfATqSgtB0w\nqYn56npuqjMzW6beK6NJwFYREc3MTKfxJbiZ1cPHir7V24HhHmCDZmbEzMx6V71XRqOA+yTdCrxS\nSoyIjzUlV2ZmXcJN8vWpNxgd18xMmJlZb6v3fUbXA48Cq+bh20gPTrVB8BmTWWu5DraPenvTHQZc\nBPwqJ20EXNasTJmZdRsHvtrq7cBwOPABYAm89aK99ZuVqU7ggmVm1jj1BqNXIuLV0oikVQB3826A\n3Xad4cBmZj2v3mB0vaRjgDUl7QZcCFzRvGyZmTVXK04CfeJZXb3B6GhgIXA38DngSnr4Da8uUGZm\njVXvg1LfBH6dP2bWJiRNBaYCjB8/vsW56Qw+mWxP9fame0TSw+WfZmfOzGqLiGkRMSkiJo0ePbrV\n2bE6OBhWVm8z3SSWPSB1Z+Bk4JxmZapXuZCa9QbX9RXV+6PXpwufJyLiJOAjtf5H0umSFki6p5A2\nUtJ0SbPy3xE5XZJOljRb0l1lb5U9KM8/S9JBA9xOMzNrY/U2000sfCZJ+jx93286A9izLO1o4NqI\n2Ay4No8D7AVslj9TgdPyekcCxwI7ANsDx5YCWKv4jMbMrPHqfTbdTwrDr5MeDfTJWv8QETdImlCW\nPAWYnIfPBGYAR+X0s/IrKm6WNFzShnne6RGxGEDSdFKAO7fOfHccP2rezHpRvb3pPtSg9Y2JiPl5\n+ElgTB7eCJhTmG9uTquWvgL3KjIz61x1BSNJX601PSJ+2t8VR0RIathTHCJiGjANYNKkSX46hJkt\nx60O7a0/vem+wLKrlc8DE4F18qdeT+XmN/LfBTn9CWBcYb6Nc1q1dDOzfvM93/ZVbzDaGJgYEV+L\niK8B2wLjI+K7EfHdfqzvcqDUI+4g4A+F9ANzr7odgedyc95VwO6SRuSOC7vntK7nSmPW3VzHl1dv\nMBoDvFoYf5Vl93sqknQucBOwuaS5kj4LnAjsJmkWsGseh/R4oYeB2aSnPHwRIHdc+B7p/Um3AceX\nOjN0s1IhdWE1s15Rb2+6s4BbJV2ax/cl9YarKiL2rzJplwrzBuk1FZWWczpwep35bCoHB7PO5Lrb\n/urtTfd9SX8iPX0B4JCI+N/mZcvMzHpJvc10AGsBSyLi58BcSZs0KU9mZg3jq6LOUO8TGI4l/Tj1\nmzlpVfxsOjMza5B6r4w+DnwMeAEgIubRvy7dZmbWh16+iqs3GL2aOxkEgKRhzcuSmZn1mnqD0QWS\nfgUMl3QYcA1+0Z6ZmTVIva+Q+DFwEXAxsDnwnYg4pZkZs2V6+dLdrNf0an3vs2u3pJWBa/LDUqc3\nP0tWiZ+rZdZ/vXpg70R9XhlFxBvAm5LWG4L8mJkNmoNQ56n3ntFS4G5Jv81vZD1Z0snNzJityBXM\nrH6dXF86Oe8DVe/jgC7JHzMzGwK91jRfMxhJGh8Rj0dEzefQ2dDptQJqZr2hr2a6y0oDki5ucl7M\nzKxH9RWMVBh+RzMzYmZmvauvYBRVhq2FevHmplkv6qW63lcwep+kJZKeB96bh5dIel7SkqHIoJlZ\nL+uVgFQzGEXEyhGxbkSsExGr5OHS+LpDlclWa/fC0O75MzPrS71du60NOQiZWbfoz8v1epIP+GZm\nzedg1CV223WGA6dZl+qF+u1gVEW3f/FmZu3EwagGByQzayfdfExyMKqg07/wTs+/Wb16ofmqXLdu\nr4NRl+rFSmpmncvByMysw3TjiaZ/Z9TlSoXWT/q2btKNB+Ne5ysjM2tbxaBTGnYgSrptPzgY9RDf\nR7JO5/K7vG7aHw5GPchByTqJy2pt3VKfHYx6WDcUYOtO5QdYl9X6dPJ+cjAq6OQvcjBc6a2duAwO\nTKffU3MwMqB7LvWtc7n8NU4n7suOCUaS9pT0oKTZko5u9PI78ctrluIZVvFj1iydflbf7jphv3bE\n74wkrQz8EtgNmAvcJunyiLivEcvvhC+qHey26wymXzN5hf3l3zDZQLjeDa12/81hRwQjYHtgdkQ8\nDCDpPGAKMKhg5MrQGNWCFFAzvfS/xfFqy7bOUKtO+XtsD9W+o1Z/P50SjDYC5hTG5wI7FGeQNBWY\nmkeXSnowD48CFtW7Iqkj0kdJlbepzfJZd3oer/hdVVtGB3h7s1dQo9w3W7/qFTT3e+xj2f3Oaz+W\nPSgVlj2ovPax7IbOT/W8Dqjcd0ow6lNETAOmladLmhkRk1qQpabpxm2C7t2uZqpW7putk74r57U5\nGp3XTunA8AQwrjC+cU4zM7Mu0CnB6DZgM0mbSFoN2A+4vMV5MjOzBumIZrqIeF3SEcBVwMrA6RFx\nb53/PuRNGEOgG7cJune7ulEnfVfOa3M0NK+KiEYuz8zMrN86pZnOzMy6mIORmZm1XNcGo2Y/PqjR\nJJ0uaYGkewppIyVNlzQr/x2R0yXp5Lxtd0maWPifg/L8syQd1IptKeRlnKTrJN0n6V5JX8npHb1d\nvU7So5LulnSnpJmtzk8tkoZLukjSA5Lul/RXrc5TJZI2z/uz9Fki6chW56saSf+Y6/Q9ks6VtMag\nFxoRXfchdXJ4CHgHsBrwF2CrVuerjzz/DTARuKeQ9iPg6Dx8NPAveXhv4E+AgB2BW3L6SODh/HdE\nHh7Rwm3aEJiYh9cB/g/YqtO3q9c/wKPAqFbno868ngkcmodXA4a3Ok915Hll4Eng7a3OS5X8bQQ8\nAqyZxy8ADh7scrv1yuitxwdFxKtA6fFBbSsibgAWlyVPIVUm8t99C+lnRXIzMFzShsAewPSIWBwR\nzwDTgT2bn/vKImJ+RNyRh58H7icV5I7eLusMktYjneT9FiAiXo2IZ1ubq7rsAjwUEY+1OiM1rAKs\nKWkVYC1g3mAX2K3BqNLjgzZqUV4GY0xEzM/DTwJj8nC17Wvb7ZY0AXg/cAtdtF09KoCrJd2eH0fU\nrjYBFgL/Lul/Jf1G0rBWZ6oO+wHntjoT1UTEE8CPgceB+cBzEXH1YJfbrcGo60S6Hu7IfviS1gYu\nBo6MiCXFaZ28XT1sp4iYCOwFHC7pb1qdoSpWITV9nxYR7wdeIDULt638o/6PARe2Oi/V5Hu8U0jB\nfiwwTNKnB7vcbg1G3fL4oKdyMxX574KcXm372m67Ja1KCkS/i4hLcnLHb1cvy2fGRMQC4FJSs3g7\nmgvMjYhb8vhFpODUzvYC7oiIp1qdkRp2BR6JiIUR8RpwCfDXg11otwajbnl80OVAqefYQcAfCukH\n5t5nO5Iuk+eTnlCxu6QR+exl95zWEpJEaq+/PyJ+WpjU0dvVyyQNk7ROaZj0XdxT+79aIyKeBOZI\n2jwn7cIgXzszBPanjZvosseBHSWtlev4LqT7wYPT6p4ZTezxsTep99ZDwLdanZ868nsuqf31NdIZ\n3WeBtwHXArOAa4CReV6RXjb4EHA3MKmwnM8As/PnkBZv006kJri7gDvzZ+9O365e/pB6qP4lf+5t\n97oFbAPMzGXwMtq4FyYwDHgaWK/Veakjr98FHiCdiJwNrD7YZfpxQGZm1nLd2kxnZmYdxMHIzMxa\nzsHIzMxazsHIzMxazsHIzMxazsGoh0j6Vn7S7l35ycA71Jj3DEl/N5T5M2s0l/nO0RGvHbfBy4/O\n/yjpKdqvSBpFeoqxWVdyme8svjLqHRsCiyLiFYCIWBQR8yR9R9Jt+b0k0/IvqpcjaVtJ1+cHY15V\neJTPl/O7iu6SdN4Qb49ZX1zmO4h/9Noj8sNKbyQ97v0a4PyIuF7SyIhYnOc5G7ggIq6QdAbwR9Kj\neq4HpkTEQkmfAvaIiM9Imgdsks86h0dnPJ7feoTLfGdxM12PiIilkrYFdgY+BJyv9Abc5yV9g1Rh\nR5Ie8XJF4V83B94DTM8nkCuTHlsE6RErv5N0GelRK2Ztw2W+szgY9ZCIeAOYAcyQdDfwOeC9pGfA\nzZF0HFD++mAB90ZEpdc1f4T08rJ9gG9J2joiXm9W/s36y2W+c/ieUY+QtLmkzQpJ2wAP5uFFuUmj\nUk+iB4HR+WYwklaV9G5JKwHjIuI64ChgPWDt5m2BWf+4zHcWXxn1jrWBUyQNB14nPf16KvAs6cm7\nT5JevbGciHg1d3c9Wek1zqsAJ5GeiH5OThNwstvPrc24zHcQd2AwM7OWczOdmZm1nIORmZm1nIOR\nmZm1nIORmZm1nIORmZm1nIORmZm1nIORmZm13P8HLozHxZxMR3EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPRUH6T-x3ia",
        "colab_type": "code",
        "outputId": "72d7a026-fbe5-40c8-aeb6-51d1301961d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "#Model training on the training set\n",
        "model1=ols(formula=\"Sales~Customers\", data=Sales_Store_merged_training).fit()\n",
        "model2=ols(formula=\"np.log(Sales)~np.log(Customers)\", data=Sales_Store_merged_training).fit()\n",
        "\n",
        "print(model1.summary())\n",
        "print(model2.summary())"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  Sales   R-squared:                       0.699\n",
            "Model:                            OLS   Adj. R-squared:                  0.699\n",
            "Method:                 Least Squares   F-statistic:                 1.523e+05\n",
            "Date:                Tue, 08 Oct 2019   Prob (F-statistic):               0.00\n",
            "Time:                        06:17:27   Log-Likelihood:            -5.6991e+05\n",
            "No. Observations:               65631   AIC:                         1.140e+06\n",
            "Df Residuals:                   65629   BIC:                         1.140e+06\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept   1115.5926     15.234     73.230      0.000    1085.734    1145.452\n",
            "Customers      7.9059      0.020    390.319      0.000       7.866       7.946\n",
            "==============================================================================\n",
            "Omnibus:                    12469.720   Durbin-Watson:                   1.446\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            40935.168\n",
            "Skew:                           0.963   Prob(JB):                         0.00\n",
            "Kurtosis:                       6.356   Cond. No.                     2.05e+03\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 2.05e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:          np.log(Sales)   R-squared:                       0.733\n",
            "Model:                            OLS   Adj. R-squared:                  0.733\n",
            "Method:                 Least Squares   F-statistic:                 1.798e+05\n",
            "Date:                Tue, 08 Oct 2019   Prob (F-statistic):               0.00\n",
            "Time:                        06:17:27   Log-Likelihood:                 12575.\n",
            "No. Observations:               65631   AIC:                        -2.515e+04\n",
            "Df Residuals:                   65629   BIC:                        -2.513e+04\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "Intercept             3.0547      0.013    227.867      0.000       3.028       3.081\n",
            "np.log(Customers)     0.8757      0.002    423.994      0.000       0.872       0.880\n",
            "==============================================================================\n",
            "Omnibus:                      741.773   Durbin-Watson:                   1.478\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              579.839\n",
            "Skew:                           0.149   Prob(JB):                    1.23e-126\n",
            "Kurtosis:                       2.649   Cond. No.                         114.\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuIB9RFcx5rQ",
        "colab_type": "code",
        "outputId": "e955ed1b-845c-41b9-ec29-1c0375c22003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Selecting the best model based the validation set\n",
        "\n",
        "##First, get the predicted value \n",
        "Prediction1=model1.predict(Sales_Store_merged_validation)\n",
        "###Model 2 took used log transformation on the dependent variable\n",
        "###Need to exponentiate the predicted value \n",
        "Prediction2=np.exp(model2.predict(Sales_Store_merged_validation))\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt( ( (predictions - targets) ** 2 ).mean() )\n",
        "\n",
        "def rmspe(predictions, targets):\n",
        "  # (np.mean(((predictions-targets)/targets)**2))**0.5\n",
        "    return np.sqrt( ( ( (predictions - targets)/targets) ** 2).mean() )\n",
        "\n",
        "##Second, get the RMSPE and RMSE\n",
        "\n",
        "Actual=Sales_Store_merged_validation[\"Sales\"]\n",
        "RMSPE1=rmspe(Prediction1, Actual)\n",
        "RMSPE2=rmspe(Prediction2, Actual)\n",
        "\n",
        "RMSE1=rmse(Prediction1, Actual)\n",
        "RMSE2=rmse(Prediction2, Actual)\n",
        "\n",
        "print(RMSPE1,RMSPE2)\n",
        "print(RMSE1,RMSE2)\n",
        "\n",
        "# The RMSPE is a more trustworthy measure to capture the accuracy of a regression model. So, based on the RMSPE value of the 2 models, we conclude that the \n",
        "# the 2nd model has a better performance (= has higher predictive power), since its RMSPE is lower, ie RMSPE2=0.193<RMSPE1=0.200."
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20039447823717005 0.19366408523982762\n",
            "1497.3942814659956 1513.855766422655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ76x7ZQfd52",
        "colab_type": "text"
      },
      "source": [
        "##Q4 [7 points]\n",
        "\n",
        "**[Feature Engineering 2]**\n",
        "\n",
        "In the business operation, stores performance can vary with time. Including the time/date information can potentially help us increase the model fit. Some typical factors we can consider are:\n",
        "\n",
        "* Year [store performance in 1992 can be different from the performance in 2010]\n",
        "* Month [For example, an ice-cream shop generally have higher sales in the summer months]\n",
        "* Day of the week [Especially, weekend v.s. weekdays might show very differen performances]\n",
        "* Important events\n",
        "\n",
        "In our dataset, we have \"DayOfWeek\", \"SchoolHoliday\" and \"StateHoliday\" to capture the last two factors. However, we do not have the \"Year\" and \"Month\".\n",
        "\n",
        "Luckily, once we specify \"Date\" as the datetime type. We can easily extract the time information. See the link [here](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties)\n",
        "\n",
        "For Q4: Do the following \n",
        "\n",
        "* Argue the pros and cons of adding Month as a categorical variable v.s. continuous variable.\n",
        "* Determine the best decision using cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ducug-3Ufdh_",
        "colab_type": "code",
        "outputId": "8450ab60-6055-4ff9-cca2-7c2e3d8b3f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Suppose x3 is a datetype column. If you want to get year information and treat it as a continuous variable in the regression, you can simply do\n",
        "#(\"y~x1+x2+x3.dt.month\",data=Data)\n",
        "model3=ols(formula=\"np.log(Sales)~np.log(Customers)+Date.dt.month\", data=Sales_Store_merged_training).fit()\n",
        "print(model3.summary())\n",
        "\n",
        "model4=ols(formula=\"np.log(Sales)~np.log(Customers)+C(Date.dt.month)\", data=Sales_Store_merged_training).fit()\n",
        "print(model4.summary())"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:          np.log(Sales)   R-squared:                       0.733\n",
            "Model:                            OLS   Adj. R-squared:                  0.733\n",
            "Method:                 Least Squares   F-statistic:                 9.016e+04\n",
            "Date:                Tue, 08 Oct 2019   Prob (F-statistic):               0.00\n",
            "Time:                        06:17:27   Log-Likelihood:                 12648.\n",
            "No. Observations:               65631   AIC:                        -2.529e+04\n",
            "Df Residuals:                   65628   BIC:                        -2.526e+04\n",
            "Df Model:                           2                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "Intercept             3.0511      0.013    227.793      0.000       3.025       3.077\n",
            "np.log(Customers)     0.8739      0.002    422.371      0.000       0.870       0.878\n",
            "Date.dt.month         0.0027      0.000     12.055      0.000       0.002       0.003\n",
            "==============================================================================\n",
            "Omnibus:                      744.081   Durbin-Watson:                   1.479\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              581.362\n",
            "Skew:                           0.149   Prob(JB):                    5.74e-127\n",
            "Kurtosis:                       2.649   Cond. No.                         159.\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:          np.log(Sales)   R-squared:                       0.738\n",
            "Model:                            OLS   Adj. R-squared:                  0.737\n",
            "Method:                 Least Squares   F-statistic:                 1.536e+04\n",
            "Date:                Tue, 08 Oct 2019   Prob (F-statistic):               0.00\n",
            "Time:                        06:17:28   Log-Likelihood:                 13187.\n",
            "No. Observations:               65631   AIC:                        -2.635e+04\n",
            "Df Residuals:                   65618   BIC:                        -2.623e+04\n",
            "Df Model:                          12                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==========================================================================================\n",
            "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------------\n",
            "Intercept                  3.0972      0.013    230.185      0.000       3.071       3.124\n",
            "C(Date.dt.month)[T.2]     -0.0046      0.003     -1.425      0.154      -0.011       0.002\n",
            "C(Date.dt.month)[T.3]      0.0187      0.003      5.859      0.000       0.012       0.025\n",
            "C(Date.dt.month)[T.4]     -0.0046      0.004     -1.277      0.202      -0.012       0.002\n",
            "C(Date.dt.month)[T.5]      0.0081      0.004      2.229      0.026       0.001       0.015\n",
            "C(Date.dt.month)[T.6]      0.0062      0.004      1.707      0.088      -0.001       0.013\n",
            "C(Date.dt.month)[T.7]      0.0137      0.004      3.793      0.000       0.007       0.021\n",
            "C(Date.dt.month)[T.8]     -0.0117      0.004     -3.233      0.001      -0.019      -0.005\n",
            "C(Date.dt.month)[T.9]     -0.0241      0.004     -6.584      0.000      -0.031      -0.017\n",
            "C(Date.dt.month)[T.10]    -0.0207      0.004     -5.683      0.000      -0.028      -0.014\n",
            "C(Date.dt.month)[T.11]     0.0321      0.004      8.697      0.000       0.025       0.039\n",
            "C(Date.dt.month)[T.12]     0.0924      0.004     24.794      0.000       0.085       0.100\n",
            "np.log(Customers)          0.8679      0.002    421.007      0.000       0.864       0.872\n",
            "==============================================================================\n",
            "Omnibus:                      722.939   Durbin-Watson:                   1.498\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              555.408\n",
            "Skew:                           0.140   Prob(JB):                    2.48e-121\n",
            "Kurtosis:                       2.646   Cond. No.                         116.\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyWbptyF4t5b",
        "colab_type": "code",
        "outputId": "3d873112-8478-43fc-cb7d-86a48135be13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Selecting the best model based the validation set\n",
        "\n",
        "##First, get the predicted value \n",
        "\n",
        "###Model 3 took used log transformation on the dependent variable\n",
        "###Need to exponentiate the predicted value \n",
        "Prediction3=np.exp(model3.predict(Sales_Store_merged_validation))\n",
        "\n",
        "###Model 4 took used log transformation on the dependent variable\n",
        "###Need to exponentiate the predicted value \n",
        "Prediction4=np.exp(model4.predict(Sales_Store_merged_validation))\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt( ( (predictions - targets) ** 2 ).mean() )\n",
        "\n",
        "def rmspe(predictions, targets):\n",
        "  # (np.mean(((predictions-targets)/targets)**2))**0.5\n",
        "    return np.sqrt( ( ( (predictions - targets)/targets) ** 2).mean() )\n",
        "\n",
        "##Second, get the RMSPE\n",
        "Actual=Sales_Store_merged_validation[\"Sales\"]\n",
        "RMSPE3=rmspe(Prediction3, Actual)\n",
        "RMSPE4=rmspe(Prediction4, Actual)\n",
        "\n",
        "print(RMSPE3,RMSPE4)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.19338148374190467 0.1931645964191939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZSqW9ZtlL07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The RMSPE is a more trustworthy measure to capture the accuracy of a regression model. So, based on the RMSPE value of the 2 models, we conclude that the \n",
        "# the 2nd model (model4) has better performance (= has higher predictive power), since its RMSPE is lower, ie RMSPE4=0.1931<RMSPE3=0.1933."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKk04DXh-yNt",
        "colab_type": "text"
      },
      "source": [
        "##Q5 [10 points]\n",
        "### [Forward Model Selection]\n",
        "\n",
        "Time to practice some model selection technique. Let's suppose that we want to determine some predictors we would like to adding based on the baseline model you chose in **Q2**. [To reduce your coding complexity, let's simply consider the following three factors]\n",
        "\n",
        "* Promo\t\n",
        "* Promo2\n",
        "* SchoolHoliday\n",
        "\n",
        "\n",
        "Based on cross-validation, choose the best model. \n",
        "* Report RMSPE of each model and the new variable to be selected in each round of model selection\n",
        "[You can do this manually]\n",
        "\n",
        "* After selecting the final model, report the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDhdVlm2nCDE",
        "colab_type": "code",
        "outputId": "40156f86-c1ee-410d-a261-cdba725c1d48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "list_ = ['Promo', 'Promo2', 'SchoolHoliday']\n",
        "RMSPE_list_ = []\n",
        "\n",
        "for item_list_ in list_:\n",
        "  model_=ols(formula=\"np.log(Sales)~np.log(Customers)\"+\"+\"+item_list_ ,data=Sales_Store_merged_training).fit()\n",
        "  Actual=Sales_Store_merged_validation[\"Sales\"]\n",
        "  Prediction_=np.exp(model_.predict(Sales_Store_merged_validation))\n",
        "  RMSPE_=rmspe(Prediction_, Actual)\n",
        "  RMSPE_list_.append(RMSPE_)\n",
        "  # print(RMSPE_)\n",
        "\n",
        "RMSPE_min = RMSPE_list_.index(min(RMSPE_list_))\n",
        "print('The variable to be selected for the 1st round of model selection is: ', list_[RMSPE_min])\n",
        "\n",
        "# We conclude that 'Promo' is the new variable that should be selected in the 1st round of model selection, since the RMSPE we obtain when we include 'Promo' in our model\n",
        "# is the minimal one, between the others we obtain for 'Promo2' and 'SchoolHoliday', respectively.\n",
        "\n",
        "list2_ = ['Promo2', 'SchoolHoliday']\n",
        "RMSPE_list_2 = []\n",
        "for item_list_ in list2_:\n",
        "  model_=ols(formula=\"np.log(Sales)~np.log(Customers)+Promo\"+\"+\"+item_list_ ,data=Sales_Store_merged_training).fit()\n",
        "  Actual_2=Sales_Store_merged_validation[\"Sales\"]\n",
        "  Prediction_2=np.exp(model_.predict(Sales_Store_merged_validation))\n",
        "  RMSPE_2=rmspe(Prediction_2, Actual_2)\n",
        "  RMSPE_list_2.append(RMSPE_2)\n",
        "  # print(RMSPE_2)\n",
        "\n",
        "RMSPE_min_2 = RMSPE_list_2.index(min(RMSPE_list_2))\n",
        "print('The variable to be selected for the 1st round of model selection is: ', list2_[RMSPE_min_2])\n",
        "\n",
        "# We conclude that 'SchoolHoliday' is the new variable that should be selected in the 2nd round of model selection, since the RMSPE we obtain when we include 'SchoolHoliday' in our\n",
        "# model is the minimal one, between the other we obtain for 'Promo2'.\n",
        "\n",
        "# Selecting and running the final model \n",
        "\n",
        "model_final = ols(formula=\"np.log(Sales)~np.log(Customers)+Promo+SchoolHoliday\",data=Sales_Store_merged_training).fit()\n",
        "Actual_final=Sales_Store_merged_testing[\"Sales\"]\n",
        "Prediction_final=np.exp(model_final.predict(Sales_Store_merged_testing))\n",
        "# print(model_final.summary())\n",
        "\n",
        "# Reporting the accuracy on the test set\n",
        "RMSPE_final=rmspe(Prediction_final, Actual_final)\n",
        "print('The final accuracy on the test set is: ', RMSPE_final)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The variable to be selected for the 1st round of model selection is:  Promo\n",
            "The variable to be selected for the 1st round of model selection is:  SchoolHoliday\n",
            "The final accuracy on the test set is:  0.18452112071723537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV2x6C2aEEos",
        "colab_type": "text"
      },
      "source": [
        "##Q6 [10 points]\n",
        "###[Ridge regression]\n",
        "\n",
        "When working with dataset (especially if we are using many predictors for a small dataset), we might encounter some serious problem with over-fitting.\n",
        "\n",
        "To demonstrate how to use Ridge regression to deal with this problem. We will explore a setting when we have very limited data. In this dataset, let's simply use the information from **Store 3** from 2013-01-02 to 2013-02-18. \n",
        "\n",
        "Again, we will split our data into three segments based on the date.\n",
        "\n",
        "* Training: Information for Store 4 before 2013-02-02\n",
        "* Validation: Information for Store 4 between 2013-02-2 and 2013-02-12-11\n",
        "* Testing: Information from Store 4 between 2013-02-12 and 2013-02-18\n",
        "\n",
        "\n",
        "Conduct ridge regression on the training set. \n",
        "\n",
        "Assume the model you want to run is \n",
        "$$log(Sale)=\\beta_0+\\beta_1 log (Customers) + \\beta_2 Customers +\\epsilon$$\n",
        "\n",
        "\n",
        "* Plot the relationship between $\\alpha$ and RMSPE on the Validation set. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Sq4_8UOE-0q",
        "colab_type": "code",
        "outputId": "1f0849c8-e5b5-4608-8712-8c6560dcc852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "filter_ = (Sales_Store_merged[\"Store\"] == 3) & ((Sales_Store_merged[\"Date\"] >= \"2013-01-02\") & (Sales_Store_merged[\"Date\"] <= \"2013-02-18\"))\n",
        "Sales_Store_merged_Ridge = Sales_Store_merged[filter_]\n",
        "Sales_Store_merged_Ridge.head()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Customers</th>\n",
              "      <th>Open</th>\n",
              "      <th>Promo</th>\n",
              "      <th>StateHoliday</th>\n",
              "      <th>SchoolHoliday</th>\n",
              "      <th>StoreType</th>\n",
              "      <th>Assortment</th>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <th>CompetitionOpenSinceMonth</th>\n",
              "      <th>CompetitionOpenSinceYear</th>\n",
              "      <th>Promo2</th>\n",
              "      <th>Promo2SinceWeek</th>\n",
              "      <th>Promo2SinceYear</th>\n",
              "      <th>PromoInterval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>962577</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2013-02-18</td>\n",
              "      <td>10793</td>\n",
              "      <td>1008</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>964807</th>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2013-02-16</td>\n",
              "      <td>4000</td>\n",
              "      <td>488</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>965922</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2013-02-15</td>\n",
              "      <td>5159</td>\n",
              "      <td>673</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>967037</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2013-02-14</td>\n",
              "      <td>5912</td>\n",
              "      <td>741</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>968152</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2013-02-13</td>\n",
              "      <td>5111</td>\n",
              "      <td>680</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Store  DayOfWeek  ... Promo2SinceYear    PromoInterval\n",
              "962577      3          1  ...          2011.0  Jan,Apr,Jul,Oct\n",
              "964807      3          6  ...          2011.0  Jan,Apr,Jul,Oct\n",
              "965922      3          5  ...          2011.0  Jan,Apr,Jul,Oct\n",
              "967037      3          4  ...          2011.0  Jan,Apr,Jul,Oct\n",
              "968152      3          3  ...          2011.0  Jan,Apr,Jul,Oct\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXcziwJoyOEQ",
        "colab_type": "code",
        "outputId": "f0f8064a-e969-48ba-f334-b1013d40928d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# Defining the training set \n",
        "\n",
        "training_filter = (Sales_Store_merged[\"Store\"] == 3) & (Sales_Store_merged[\"Date\"] < \"2013-02-02\")\n",
        "Sales_Store_merged_Ridge_training = Sales_Store_merged_Ridge[training_filter]\n",
        "Sales_Store_merged_Ridge_training.head()\n",
        "\n",
        "# Defining the validation set \n",
        "\n",
        "validation_filter = (Sales_Store_merged[\"Store\"] == 3) & ((Sales_Store_merged[\"Date\"] >= \"2013-02-02\") & (Sales_Store_merged[\"Date\"] <= \"2013-02-12\"))\n",
        "Sales_Store_merged_Ridge_validation = Sales_Store_merged_Ridge[validation_filter]\n",
        "Sales_Store_merged_Ridge_validation.head()\n",
        "\n",
        "# Defining the testing set \n",
        "\n",
        "testing_filter = (Sales_Store_merged[\"Store\"] == 3) & ((Sales_Store_merged[\"Date\"] >= \"2013-02-12\") & (Sales_Store_merged[\"Date\"] <= \"2013-02-18\"))\n",
        "Sales_Store_merged_Ridge_testing = Sales_Store_merged_Ridge[testing_filter]\n",
        "Sales_Store_merged_Ridge_testing.head()\n",
        "\n",
        "# Running the normal model, ie the non-regularized model, to find the starting parameters \n",
        "\n",
        "model_ = ols(formula=\"np.log(Sales)~np.log(Customers)+Customers\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "start_params=model_.params\n",
        "\n",
        "# Iterating over a range of alphas to come up with the alpha that gives us the lowest RMSPE. After a few trials we concluded that the interval [0, 0.10] is the one that \n",
        "# includes the value of alpha that minimizes the RMSPE of our model\n",
        "\n",
        "RMSPE_list_ = []\n",
        "alpha_range = np.linspace(0, 0.10, num=250)\n",
        "\n",
        "for alpha_ in alpha_range:\n",
        "  model_ = ols(formula=\"np.log(Sales)~np.log(Customers)+Customers\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "  start_params=model_.params\n",
        "  model_final_=ols(formula=\"np.log(Sales)~np.log(Customers)+Customers\",data=Sales_Store_merged_Ridge_training).fit_regularized(start_params=start_params, alpha=alpha_, L1_wt=1)\n",
        "  Prediction_final=np.exp(model_final_.predict(Sales_Store_merged_Ridge_validation))\n",
        "  ## Getting the RMSPE\n",
        "  Actual_final=Sales_Store_merged_Ridge_validation[\"Sales\"]\n",
        "  RMSPE_final= (np.mean(((Prediction_final-Actual_final)/Actual_final)**2))**0.5\n",
        "  RMSPE_list_.append(RMSPE_final)\n",
        "  # print(RMSPE_final)\n",
        "\n",
        "plt.plot(alpha_range, RMSPE_list_)\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"RMSPE\")\n",
        "plt.show()\n",
        "\n",
        "RMSPE_min = RMSPE_list_.index(min(RMSPE_list_))\n",
        "print('The variable to be selected for the alpha is: ', alpha_range[RMSPE_min])\n",
        "\n",
        "#####RMSPE over different intervals\n",
        "# -------RMSPE--------|--------Interval-------\n",
        "# 0.06827309236947787 | (-1,1)\n",
        "# 0.07148594377510031 | (-1, 0.15)\n",
        "# 0.06987951807228915 | (-0.2, 0.15)\n",
        "# 0.06987951807228915 | (0, 0.15)\n",
        "# 0.06987951807228916 | (0, 0.10)\n",
        "\n",
        "# Now that we found the alpha that minimizes the RMSPE we run the model using Ridge regression on the training set\n",
        "model_ = ols(formula=\"np.log(Sales)~np.log(Customers)+Customers\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "start_params=model_.params\n",
        "model_final_=ols(formula=\"np.log(Sales)~np.log(Customers)+Customers\",data=Sales_Store_merged_Ridge_training).fit_regularized(start_params=start_params, alpha=0.06987951807228916, L1_wt=1)\n",
        "Prediction_final=np.exp(model_final_.predict(Sales_Store_merged_Ridge_validation))\n",
        "## Getting the RMSPE\n",
        "Actual_final=Sales_Store_merged_Ridge_validation[\"Sales\"]\n",
        "RMSPE_final= (np.mean(((Prediction_final-Actual_final)/Actual_final)**2))**0.5\n",
        "\n",
        "# Selecting and running the final model \n",
        "\n",
        "model_final = ols(formula=\"np.log(Sales)~np.log(Customers)+Customers\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "Actual_final=Sales_Store_merged_Ridge_testing[\"Sales\"]\n",
        "Prediction_final=np.exp(model_final.predict(Sales_Store_merged_Ridge_testing))\n",
        "\n",
        "# Reporting the accuracy on the test set\n",
        "RMSPE_final=rmspe(Prediction_final, Actual_final)\n",
        "print('The final accuracy on the test set is: ', RMSPE_final)\n",
        "\n",
        "# RMSPE_list_ = []\n",
        "# alpha_range = np.linspace(0, 10, num=250)\n",
        "\n",
        "# for alpha_ in alpha_range:\n",
        "#   model_ = ols(formula=\"np.log(Sales)~np.log(Customers)+Customers\",data=Sales_Store_merged_Ridge_validation).fit()\n",
        "#   start_params=model_.params\n",
        "#   model_final_=ols(formula=\"np.log(Sales)~np.log(Customers)+Customers\",data=Sales_Store_merged_Ridge_validation).fit_regularized(start_params=start_params, alpha=alpha_, L1_wt=1)\n",
        "#   Prediction_final=np.exp(model_final_.predict(Sales_Store_merged_Ridge_validation))\n",
        "#   ## Getting the RMSPE\n",
        "#   Actual_final=Sales_Store_merged_Ridge_validation[\"Sales\"]\n",
        "#   RMSPE_final= (np.mean(((Prediction_final-Actual_final)/Actual_final)**2))**0.5\n",
        "#   RMSPE_list_.append(RMSPE_final)\n",
        "#   # print(RMSPE_final)\n",
        "\n",
        "# plt.plot(alpha_range, RMSPE_list_)\n",
        "# plt.xlabel(\"alpha\")\n",
        "# plt.ylabel(\"RMSPE\")\n",
        "# plt.show()\n",
        "\n",
        "# RMSPE_min = RMSPE_list_.index(min(RMSPE_list_))\n",
        "# print('The variable to be selected for the alpha is: ', alpha_range[RMSPE_min])"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzDpysDAzs44cLMzMrCMHCzMz6+j/A0sY7NOGwtbUAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The variable to be selected for the alpha is:  0.06987951807228916\n",
            "The final accuracy on the test set is:  0.09868613781369324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNxBSgjGNSDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Make a multiple-histogram of data-sets with different length.\n",
        "# x_multi = [np.random.randn(n) for n in [10000, 5000, 2000]]\n",
        "# ax3.hist(x_multi, n_bins, histtype='bar')\n",
        "# ax3.set_title('different sample sizes')\n",
        "\n",
        "# fig.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsl2cJPei2M3",
        "colab_type": "text"
      },
      "source": [
        "##Q7. [13 points]\n",
        "###[Standardization in ridge regression]\n",
        "The previous model gives an okay solution. However, one thing to notice is that the scale of variations could also affect $\\beta$ values, which can cause factors to be discriminated simply based on whether the values are small or large. In addition, we do not want $\\beta_0$ to be penalized.\n",
        "\n",
        "As a result, people usually standardize the variables. Especially, for Ridge regression, a common practice is to do the following:\n",
        "\n",
        "**On the training set:**\n",
        "\n",
        "1. Standardize each independent variable using \n",
        "\n",
        "$$\\frac{x_k-\\bar{x_k}}{s_{x_k}}$$\n",
        "\n",
        "where $\\bar{x}$ and $s_x$ are the mean and the standard deviation of variable $x_k$ on the training set. \n",
        "\n",
        "\n",
        "2. Re-center the dependent variable using \n",
        "\n",
        "$$y-\\bar{y}$$\n",
        "\n",
        "where $\\bar{y}$ is the mean of $y$ on the training set.\n",
        "\n",
        "This allows us to tease out the impact of $\\beta_0$ in the regulation\n",
        "\n",
        "3. Regress the de-centered depedent variable on the standardized independent variables without intercept. [you can simply use \"y~x1+x2-1\" to not estimate the intercept]\n",
        "\n",
        "**On the validation and testing set:**\n",
        "\n",
        "When you do the prediction on the validation set, we will have\n",
        "\n",
        "$$\\hat{y}=\\bar{y}+\\sum \\hat{\\beta_k} {\\frac{x-\\bar{x_k}}{s_{x_k}}}$$\n",
        "\n",
        "where $\\bar{y}$ is the mean of $y$ on the **training set**, $\\bar{x_k}$ is the mean of $x_k$ on the **training set**, while $s_{x_k}$ is the standard deviation of $x_k$ on the **training set**.\n",
        "\n",
        "\n",
        "\n",
        "* Use this method to re-do Q6. \n",
        "* Determine the best model based on Q6 and Q7. Report the accuracy on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKeiFfRK1VHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Original model\n",
        "model1 = ols(formula=\"np.log(Sales)~np.log(Customers)+Customers\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "prediction1 = np.exp(model1.predict())\n",
        "# prediction1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0fOKxLRs8Tc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "b43b55cb-d352-4d8f-8034-1acdbaa2c4f1"
      },
      "source": [
        "#demean logSale\n",
        "logSale_mean=np.mean(np.log(Sales_Store_merged_Ridge_training[\"Sales\"]))\n",
        "Sales_Store_merged_Ridge_training=Sales_Store_merged_Ridge_training.assign(logSales_demean=lambda x:np.log(x.Sales)-logSale_mean)\n",
        "\n",
        "#standardize the independent variables\n",
        "logCustomers_mean=np.mean(np.log(Sales_Store_merged_Ridge_training[\"Customers\"]))\n",
        "logCustomers_Std=np.std(np.log(Sales_Store_merged_Ridge_training[\"Customers\"]))\n",
        "Sales_Store_merged_Ridge_training=Sales_Store_merged_Ridge_training.assign(logCustomers_standardized=lambda x:(np.log(x.Customers)-logCustomers_mean)/logCustomers_Std)\n",
        "\n",
        "Customers_mean=np.mean(Sales_Store_merged_Ridge_training[\"Customers\"])\n",
        "Customers_Std=np.std(Sales_Store_merged_Ridge_training[\"Customers\"])\n",
        "Sales_Store_merged_Ridge_training=Sales_Store_merged_Ridge_training.assign(Customers_standardized=lambda x:(x.Customers-Customers_mean)/Customers_Std)\n",
        "\n",
        "Sales_Store_merged_Ridge_training.head()"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Customers</th>\n",
              "      <th>Open</th>\n",
              "      <th>Promo</th>\n",
              "      <th>StateHoliday</th>\n",
              "      <th>SchoolHoliday</th>\n",
              "      <th>StoreType</th>\n",
              "      <th>Assortment</th>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <th>CompetitionOpenSinceMonth</th>\n",
              "      <th>CompetitionOpenSinceYear</th>\n",
              "      <th>Promo2</th>\n",
              "      <th>Promo2SinceWeek</th>\n",
              "      <th>Promo2SinceYear</th>\n",
              "      <th>PromoInterval</th>\n",
              "      <th>logSales_demean</th>\n",
              "      <th>logCustomers_standardized</th>\n",
              "      <th>Customers_standardized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>981532</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2013-02-01</td>\n",
              "      <td>6725</td>\n",
              "      <td>806</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>0.095090</td>\n",
              "      <td>0.721619</td>\n",
              "      <td>0.686587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982647</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2013-01-31</td>\n",
              "      <td>5682</td>\n",
              "      <td>648</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>-0.073439</td>\n",
              "      <td>-0.313111</td>\n",
              "      <td>-0.412467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983762</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2013-01-30</td>\n",
              "      <td>4920</td>\n",
              "      <td>647</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>-0.217434</td>\n",
              "      <td>-0.320435</td>\n",
              "      <td>-0.419423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984877</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2013-01-29</td>\n",
              "      <td>5375</td>\n",
              "      <td>711</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>-0.128984</td>\n",
              "      <td>0.126885</td>\n",
              "      <td>0.025763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985992</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2013-01-28</td>\n",
              "      <td>5352</td>\n",
              "      <td>722</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>-0.133272</td>\n",
              "      <td>0.199692</td>\n",
              "      <td>0.102279</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Store  DayOfWeek  ... logCustomers_standardized  Customers_standardized\n",
              "981532      3          5  ...                  0.721619                0.686587\n",
              "982647      3          4  ...                 -0.313111               -0.412467\n",
              "983762      3          3  ...                 -0.320435               -0.419423\n",
              "984877      3          2  ...                  0.126885                0.025763\n",
              "985992      3          1  ...                  0.199692                0.102279\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX4kn5B4tC0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2=ols(formula=\"logSales_demean~logCustomers_standardized+Customers_standardized-1\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "# prediction2=np.exp(model2.predict()+logSale_mean)\n",
        "# prediction2-prediction1\n",
        "\n",
        "# We need to follow the same procedure for the validation set:\n",
        "\n",
        "#demean logSale\n",
        "logSale_mean=np.mean(np.log(Sales_Store_merged_Ridge_validation[\"Sales\"]))\n",
        "Sales_Store_merged_Ridge_validation=Sales_Store_merged_Ridge_validation.assign(logSales_demean=lambda x:np.log(x.Sales)-logSale_mean)\n",
        "\n",
        "#standardize the independent variables\n",
        "logCustomers_mean=np.mean(np.log(Sales_Store_merged_Ridge_validation[\"Customers\"]))\n",
        "logCustomers_Std=np.std(np.log(Sales_Store_merged_Ridge_validation[\"Customers\"]))\n",
        "Sales_Store_merged_Ridge_validation=Sales_Store_merged_Ridge_validation.assign(logCustomers_standardized=lambda x:(np.log(x.Customers)-logCustomers_mean)/logCustomers_Std)\n",
        "\n",
        "Customers_mean=np.mean(Sales_Store_merged_Ridge_validation[\"Customers\"])\n",
        "Customers_Std=np.std(Sales_Store_merged_Ridge_validation[\"Customers\"])\n",
        "Sales_Store_merged_Ridge_validation=Sales_Store_merged_Ridge_validation.assign(Customers_standardized=lambda x:(x.Customers-Customers_mean)/Customers_Std)\n",
        "\n",
        "# Now we can predict \n",
        "\n",
        "Prediction_final=np.exp(model2.predict(Sales_Store_merged_Ridge_validation)+logSale_mean)\n",
        "## Getting the RMSPE\n",
        "Actual_final=Sales_Store_merged_Ridge_validation[\"Sales\"]\n",
        "RMSPE_final= (np.mean(((Prediction_final-Actual_final)/Actual_final)**2))**0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqjzbALL0FfO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "72352293-0d07-4137-b235-90a8f79a0367"
      },
      "source": [
        "#demean logSale\n",
        "logSale_mean=np.mean(np.log(Sales_Store_merged_Ridge_training[\"Sales\"]))\n",
        "Sales_Store_merged_Ridge_training=Sales_Store_merged_Ridge_training.assign(logSales_demean=lambda x:np.log(x.Sales)-logSale_mean)\n",
        "\n",
        "#standardize the independent variables\n",
        "logCustomers_mean=np.mean(np.log(Sales_Store_merged_Ridge_training[\"Customers\"]))\n",
        "logCustomers_Std=np.std(np.log(Sales_Store_merged_Ridge_training[\"Customers\"]))\n",
        "Sales_Store_merged_Ridge_training=Sales_Store_merged_Ridge_training.assign(logCustomers_standardized=lambda x:(np.log(x.Customers)-logCustomers_mean)/logCustomers_Std)\n",
        "\n",
        "Customers_mean=np.mean(Sales_Store_merged_Ridge_training[\"Customers\"])\n",
        "Customers_Std=np.std(Sales_Store_merged_Ridge_training[\"Customers\"])\n",
        "Sales_Store_merged_Ridge_training=Sales_Store_merged_Ridge_training.assign(Customers_standardized=lambda x:(x.Customers-Customers_mean)/Customers_Std)\n",
        "\n",
        "# Running the normal model, ie the non-regularized model, to find the starting parameters \n",
        "\n",
        "model_ = ols(formula=\"logSales_demean~logCustomers_standardized+Customers_standardized-1\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "start_params=model_.params\n",
        "\n",
        "# Iterating over a range of alphas to come up with the alpha that gives us the lowest RMSPE. After a few trials we concluded that the interval [0, 0.10] is the one that \n",
        "# includes the value of alpha that minimizes the RMSPE of our model\n",
        "\n",
        "RMSPE_list_ = []\n",
        "alpha_range = np.linspace(-0.2, 0.10, num=250)\n",
        "\n",
        "for alpha_ in alpha_range:\n",
        "  model_ = ols(formula=\"logSales_demean~logCustomers_standardized+Customers_standardized-1\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "  start_params=model_.params\n",
        "  model_final_=ols(formula=\"logSales_demean~logCustomers_standardized+Customers_standardized-1\",data=Sales_Store_merged_Ridge_training).fit_regularized(start_params=start_params, alpha=alpha_, L1_wt=1)\n",
        "  Prediction_final=np.exp(model_final_.predict(Sales_Store_merged_Ridge_validation)+logSale_mean)\n",
        "  ## Getting the RMSPE\n",
        "  Actual_final=Sales_Store_merged_Ridge_validation[\"Sales\"]\n",
        "  RMSPE_final= (np.mean(((Prediction_final-Actual_final)/Actual_final)**2))**0.5\n",
        "  RMSPE_list_.append(RMSPE_final)\n",
        "  # print(RMSPE_final)\n",
        "\n",
        "plt.plot(alpha_range, RMSPE_list_)\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"RMSPE\")\n",
        "plt.show()\n",
        "\n",
        "RMSPE_min = RMSPE_list_.index(min(RMSPE_list_))\n",
        "print('The variable to be selected for the alpha is: ', alpha_range[RMSPE_min])\n",
        "\n",
        "#####RMSPE over different intervals\n",
        "# -------RMSPE--------|--------Interval-------\n",
        "# 0.06827309236947787 | (-1,1)\n",
        "# 0.07148594377510031 | (-1, 0.15)\n",
        "# 0.06987951807228915 | (-0.2, 0.15)\n",
        "# 0.06987951807228915 | (0, 0.15)\n",
        "# 0.06987951807228916 | (0, 0.10)\n",
        "\n",
        "# Now that we found the alpha that minimizes the RMSPE we run the model using Ridge regression on the training set\n",
        "model_ = ols(formula=\"logSales_demean~logCustomers_standardized+Customers_standardized-1\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "start_params=model_.params\n",
        "model_final_= ols(formula=\"logSales_demean~logCustomers_standardized+Customers_standardized-1\",data=Sales_Store_merged_Ridge_training).fit_regularized(start_params=start_params, alpha=0.06987951807228916, L1_wt=1)\n",
        "Prediction_final=np.exp(model_final_.predict(Sales_Store_merged_Ridge_validation)+logSale_mean)\n",
        "## Getting the RMSPE\n",
        "Actual_final = Sales_Store_merged_Ridge_validation[\"Sales\"]\n",
        "RMSPE_final = (np.mean(((Prediction_final-Actual_final)/Actual_final)**2))**0.5\n",
        "\n",
        "# We need to follow the same procedure for the validation set:\n",
        "\n",
        "#demean logSale\n",
        "logSale_mean=np.mean(np.log(Sales_Store_merged_Ridge_testing[\"Sales\"]))\n",
        "Sales_Store_merged_Ridge_testing=Sales_Store_merged_Ridge_testing.assign(logSales_demean=lambda x:np.log(x.Sales)-logSale_mean)\n",
        "\n",
        "#standardize the independent variables\n",
        "logCustomers_mean=np.mean(np.log(Sales_Store_merged_Ridge_testing[\"Customers\"]))\n",
        "logCustomers_Std=np.std(np.log(Sales_Store_merged_Ridge_testing[\"Customers\"]))\n",
        "Sales_Store_merged_Ridge_testing=Sales_Store_merged_Ridge_testing.assign(logCustomers_standardized=lambda x:(np.log(x.Customers)-logCustomers_mean)/logCustomers_Std)\n",
        "\n",
        "Customers_mean=np.mean(Sales_Store_merged_Ridge_testing[\"Customers\"])\n",
        "Customers_Std=np.std(Sales_Store_merged_Ridge_testing[\"Customers\"])\n",
        "Sales_Store_merged_Ridge_testing=Sales_Store_merged_Ridge_testing.assign(Customers_standardized=lambda x:(x.Customers-Customers_mean)/Customers_Std)\n",
        "\n",
        "# Now we can predict \n",
        "\n",
        "# Selecting and running the final model \n",
        "model_final = ols(formula=\"logSales_demean~logCustomers_standardized+Customers_standardized-1\",data=Sales_Store_merged_Ridge_training).fit()\n",
        "Actual_final=Sales_Store_merged_Ridge_testing[\"Sales\"]\n",
        "Prediction_final=np.exp(model_final.predict(Sales_Store_merged_Ridge_testing)+logSale_mean)\n",
        "\n",
        "# Reporting the accuracy on the test set\n",
        "RMSPE_final=rmspe(Prediction_final, Actual_final)\n",
        "print('The final accuracy on the test set is: ', RMSPE_final)\n",
        "\n",
        "print('This model has better performance (predictive power) than the one of Q6, since the final accuracy on the test set for the model of Q6 is:  0.09868613781369324 \\\n",
        "      (=RMSPE_model_Q6), and the the performance of this model is 0.04526045238754683 (=RMSPE_model_Q7). We know that the smallest the RMSPE the higher the accuracy of the \\\n",
        "      regression model, so the better the model.')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCSJ/w+zuyFfi9JT+AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The variable to be selected for the alpha is:  -0.0012048192771084154\n",
            "The final accuracy on the test set is:  0.04526045238754683\n",
            "This model has better performance (predictive power) than the one of Q6, since the final accuracy on the test set for the model of Q6 is:  0.09868613781369324       (=RMSPE_model_Q6), and the the performance of this model is 0.04526045238754683 (=RMSPE_model_Q7). We know that the smallest the RMSPE the higher the accuracy of the       regression model, so the better the model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK1irSsxs7_p",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrK-uvp_YgAO",
        "colab_type": "text"
      },
      "source": [
        "##Q*. Bonus question [3 points]\n",
        "\n",
        "Do the task in Q4 but with the following factors with the help of `for` loop\n",
        "\n",
        "* Promo\n",
        "* SchoolHoliday\n",
        "* C(Store)\n",
        "* C(DayOfWeek)\n",
        "* C(Store)#np.log(Customers)\n",
        "* C(Month)\n",
        "* C(Year)\n",
        "* C(StateHoliday)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U98v7c38Gofv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "819add43-3e94-4b2f-ea0f-bfab2d2db479"
      },
      "source": [
        "list_ = ['Promo', 'SchoolHoliday', 'C(Store)', 'C(DayOfWeek)', 'C(Store)*np.log(Customers)', 'C(Date.dt.month)', 'C(Date.dt.year)', 'C(StateHoliday)']\n",
        "RMSPE_list_ = []\n",
        "RMSPE_min_list = []\n",
        "\n",
        "for item_list_ in list_:\n",
        "  model_=ols(formula=\"np.log(Sales)~np.log(Customers)\"+\"+\"+item_list_ ,data=Sales_Store_merged_training).fit()\n",
        "  Actual=Sales_Store_merged_validation[\"Sales\"]\n",
        "  Prediction_=np.exp(model_.predict(Sales_Store_merged_validation))\n",
        "  RMSPE_=rmspe(Prediction_, Actual)\n",
        "  RMSPE_list_.append(RMSPE_)\n",
        "  # print(RMSPE_)\n",
        "  RMSPE_max = RMSPE_list_.index(max(RMSPE_list_))\n",
        "  RMSPE_min = RMSPE_list_.index(min(RMSPE_list_))\n",
        "  RMSPE_min_list.append(RMSPE_min)\n",
        "  list_.remove(list_[RMSPE_max])\n",
        "\n",
        "print('The variable to be selected for model selection is: ', [list_[i] for i in RMSPE_min_list])\n",
        "\n",
        "# We conclude that 'SchoolHoliday', 'C(DayOfWeek)', 'C(Store)*np.log(Customers)', 'C(Date.dt.month)' are the new variables that should be selected in the model selection, since \n",
        "# the RMSPEs we obtain when we include 'SchoolHoliday', 'C(DayOfWeek)', 'C(Store)*np.log(Customers)', 'C(Date.dt.month)' in our model are the minimal one, between the others we\n",
        "# obtain for 'Promo', 'SchoolHoliday', 'C(Store)', 'C(Date.dt.year)', 'C(StateHoliday)', respectively."
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The variable to be selected for model selection is:  ['C(DayOfWeek)', 'C(Store)*np.log(Customers)', 'C(Date.dt.month)', 'C(Date.dt.month)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_JDubCY7sTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}